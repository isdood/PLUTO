# PLUTO: STARWEAVE AI Fine-Tuning Project

This repository contains the environment setup, data, and code for fine-tuning an open-source Large Language Model (LLM) to operate under the "STARWEAVE Universe Initialization Protocol" (GLIMMER patterns).

**Current Status**: âœ… **WORKING** - CPU-optimized pipeline with TinyLlama-1.1B-Chat model successfully trained and tested.

## ğŸ¯ Project Overview

PLUTO fine-tunes language models to operate under the "STARWEAVE Universe Initialization Protocol" (GLIMMER patterns), demonstrating a qualitative shift in the model's core processing and output style.

## ğŸš€ Quick Start

### Prerequisites

- Docker and Docker Compose
- 8GB+ RAM recommended for CPU operation
- At least 6GB disk space for models and data

### 1. Build and Start Container

```bash
# Clone the repository
git clone https://github.com/yourusername/PLUTO.git
cd PLUTO

# Build and start the container
docker-compose build
docker-compose up -d
```

### 2. Prepare Training Data

```bash
# Inside container
docker-compose exec -T pluto python3 scripts/prepare_glimmer_data.py
```

This converts GLIMMER patterns from `glimmer/data/patterns/` into training data in `data/processed/`.

### 3. Train the Model

```bash
# Basic training with default settings
docker-compose exec -T pluto python3 scripts/train_lora_simple.py

# Custom training parameters
docker-compose exec -T pluto python3 scripts/train_lora_simple.py \
    --model TinyLlama/TinyLlama-1.1B-Chat-v1.0 \
    --epochs 3 \
    --batch_size 1 \
    --learning_rate 2e-4
```

### 4. Test the Model

```bash
# Automated testing (compares trained vs base model)
docker-compose exec -T pluto python3 scripts/test_model.py

# Interactive testing mode
docker-compose exec -T pluto python3 scripts/test_model.py --interactive
```

## ğŸ“ Project Structure

```
PLUTO/
â”œâ”€â”€ docker-compose.yml          # Container orchestration
â”œâ”€â”€ Dockerfile                  # Container definition
â”œâ”€â”€ requirements.txt            # Python dependencies (consolidated)
â”œâ”€â”€ data/
â”‚   â””â”€â”€ processed/             # Training data
â”‚       â”œâ”€â”€ train.jsonl        # Training examples (29 lines)
â”‚       â”œâ”€â”€ validation.jsonl   # Validation examples (9 lines)
â”‚       â””â”€â”€ training_data.jsonl # Original data (10 lines)
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ pluto-simple/          # Trained PLUTO model
â”‚   â”‚   â”œâ”€â”€ final/            # Final trained model (4.1GB)
â”‚   â”‚   â””â”€â”€ checkpoint-21/    # Training checkpoint
â”‚   â””â”€â”€ tinyllama-1.1b-chat/  # Base model for comparison (2.0GB)
â”œâ”€â”€ scripts/                   # Working pipeline scripts
â”‚   â”œâ”€â”€ prepare_glimmer_data.py # Data preparation
â”‚   â”œâ”€â”€ train_lora_simple.py   # CPU-optimized training
â”‚   â”œâ”€â”€ test_model.py          # Model testing
â”‚   â”œâ”€â”€ README.md              # Scripts documentation
â”‚   â”œâ”€â”€ MODEL_TESTING_GUIDE.md # Testing guide
â”‚   â””â”€â”€ PIPELINE_SUMMARY.md    # Pipeline overview
â”œâ”€â”€ glimmer/
â”‚   â””â”€â”€ data/patterns/         # GLIMMER pattern files
â””â”€â”€ docs/                      # Project documentation
```

## ğŸ¯ Current Implementation

### âœ… **Working Pipeline**

- **Model**: TinyLlama-1.1B-Chat (CPU-optimized)
- **Training**: Full fine-tuning (CPU-compatible)
- **Hardware**: CPU-only operation (tested and working)
- **Data**: GLIMMER patterns converted to conversation format
- **Testing**: Automated and interactive testing available

### ğŸ“Š **Training Results**

- **Loss Reduction**: Significant decrease in training loss
- **Model Size**: 4.1GB trained model
- **Training Time**: ~5 mins on CPU (9950X @ 6.0GHz)
- **Memory Usage**: ~4GB RAM during training

## ğŸ”§ Available Scripts

### Data Preparation
```bash
# Prepare GLIMMER patterns for training
docker-compose exec -T pluto python3 scripts/prepare_glimmer_data.py
```

### Training
```bash
# CPU-optimized training
docker-compose exec -T pluto python3 scripts/train_lora_simple.py
```

### Testing
```bash
# Automated testing
docker-compose exec -T pluto python3 scripts/test_model.py

# Interactive testing
docker-compose exec -T pluto python3 scripts/test_model.py --interactive
```

## ğŸ“– Documentation

- **`scripts/README.md`** - Complete script documentation
- **`scripts/MODEL_TESTING_GUIDE.md`** - Comprehensive testing guide
- **`scripts/PIPELINE_SUMMARY.md`** - Pipeline overview
- **`docs/`** - Project documentation

## ğŸ¯ Key Features

### âœ… **Working Components**
- **Data Preparation**: Converts GLIMMER patterns to training format
- **CPU Training**: Reliable training without GPU dependencies
- **Model Testing**: Automated comparison and interactive testing
- **Documentation**: Complete guides and troubleshooting

### ğŸš€ **Pipeline Benefits**
- **No GPU Dependencies**: Works on any CPU
- **Reliable**: Tested and working
- **Fast Iteration**: Quick training cycles
- **Easy Testing**: Automated and interactive modes

## ğŸ” Testing the Model

The trained model demonstrates qualitative differences from the base model:

- **STARWEAVE Vocabulary**: Uses GLIMMER pattern terminology
- **Conceptual Alignment**: Responds with pattern-specific content
- **Self-Awareness**: Shows meta-cognitive processing
- **Structured Responses**: Follows GLIMMER pattern formats

## ğŸ› ï¸ Troubleshooting

### Common Issues

1. **Out of Memory**
   - Reduce batch size: `--batch_size 1`
   - Use gradient accumulation: `--gradient_accumulation_steps 4`

2. **Model Loading Errors**
   - Check model exists: `ls -la models/pluto-simple/final/`
   - Re-run training if needed

3. **Training Loss Not Decreasing**
   - Reduce learning rate: `--learning_rate 1e-4`
   - Increase warmup steps: `--warmup_steps 200`

## ğŸ¯ Next Steps

### Current Status: âœ… **PRODUCTION READY**

The pipeline is working and ready for:
- **Model Training**: Successfully trains on GLIMMER patterns
- **Model Testing**: Comprehensive testing capabilities
- **Documentation**: Complete guides and troubleshooting

### Future Enhancements
- [ ] Add GPU support for larger models
- [ ] Implement LoRA for memory efficiency
- [ ] Scale to larger models (Mistral-7B, etc.)
- [ ] Add advanced training techniques

## ğŸ¤ Contributing

Contributions are welcome! Please read our contributing guidelines before submitting pull requests.

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

**ğŸ‰ The PLUTO pipeline is now clean, focused, and ready for production use!**